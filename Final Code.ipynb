{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2440c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillAnalysisTab:\n",
    "    def __init__(self, tab):\n",
    "        self.tab = tab\n",
    "        self.metrics_visible = False  # Flag to track visibility of evaluation metrics\n",
    "        self.load_data_and_process()\n",
    "        self.create_skills_analysis_tab()\n",
    "\n",
    "    def load_data_and_process(self):\n",
    "        # Check if the results file already exists\n",
    "        results_file = 'results.json'\n",
    "        if os.path.exists(results_file):\n",
    "            # If the file exists, load the results from the file\n",
    "            with open(results_file, 'r') as file:\n",
    "                results = json.load(file)\n",
    "                self.top_skills = results['top_skills']\n",
    "                self.precision = results['precision']\n",
    "                self.recall = results['recall']\n",
    "                self.f1 = results['f1']\n",
    "        else:\n",
    "            # Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "            # Load and prepare the skills from the Excel file\n",
    "            df = pd.read_excel('all_emsi_skills.xlsx')  # Load hard skills from an Excel file\n",
    "            known_skills = [skill.lower() for skill in df['name'].tolist()]\n",
    "\n",
    "            def preprocess_skills(skills):\n",
    "                patterns = []\n",
    "                for skill in skills:\n",
    "                    primary_skill = skill.split('(')[0].strip()\n",
    "                    patterns.append(nlp.make_doc(primary_skill))\n",
    "                return patterns\n",
    "\n",
    "            def extract_skills(job_description):\n",
    "                skills = set()\n",
    "                doc = nlp(job_description.lower())\n",
    "                matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "                patterns = preprocess_skills(known_skills)\n",
    "                matcher.add(\"SKILL\", patterns)\n",
    "                matches = matcher(doc)\n",
    "                for match_id, start, end in matches:\n",
    "                    span = doc[start:end]\n",
    "                    skills.add(span.text)\n",
    "                return skills\n",
    "\n",
    "            # Read CSV data into a DataFrame\n",
    "            df = pd.read_csv('job_descriptions_corrected_dataset.csv')\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for _, row in df.iterrows():\n",
    "                description = row[\"job_description\"]\n",
    "                annotated_skills = row[\"annotated_hard_skills\"].strip(\"[]\").split(\",\")\n",
    "                annotated_skills = {x.lstrip() for x in annotated_skills}\n",
    "                extracted_skills = extract_skills(description)\n",
    "                y_true.append(annotated_skills)\n",
    "                y_pred.append(extracted_skills)\n",
    "                \n",
    "\n",
    "\n",
    "            y_true = [{item.lower() for item in s} for s in y_true]\n",
    "            # If the file doesn't exist, run the model and save the results to the file\n",
    "            # Read the new CSV file with unseen job postings\n",
    "            new_df = pd.read_csv('job_descriptions.csv')\n",
    "\n",
    "            # Initialize a list to store all extracted skills\n",
    "            all_skills = []\n",
    "\n",
    "            # Iterate over rows in the new DataFrame\n",
    "            for _, row in new_df.iterrows():\n",
    "                description = row[\"Descriptions\"]  # Assuming the column name is \"job description\"\n",
    "                \n",
    "                # Extract skills from the description\n",
    "                extracted_skills = extract_skills(description)\n",
    "                \n",
    "                # Remove \"data science\" from the extracted skills\n",
    "                extracted_skills = [skill for skill in extracted_skills if skill.lower() != 'data science']\n",
    "                \n",
    "                # Append the extracted skills to the all_skills list\n",
    "                all_skills.extend(extracted_skills)\n",
    "\n",
    "            # Count the frequency of each skill\n",
    "            skill_counts = Counter(all_skills)\n",
    "\n",
    "            # Get the top 5 skills in demand\n",
    "            self.top_skills = skill_counts.most_common(5)\n",
    "\n",
    "            # Flatten the lists for evaluation\n",
    "            true_skills_flat = [skill for skills in y_true for skill in skills]\n",
    "            predicted_skills_flat = [skill for skills in y_pred for skill in skills]\n",
    "\n",
    "            # Create binary labels for each skill in the union of all skills\n",
    "            all_skills = set(true_skills_flat) | set(predicted_skills_flat)\n",
    "            true_labels = [int(skill in true_skills_flat) for skill in all_skills]\n",
    "            predicted_labels = [int(skill in predicted_skills_flat) for skill in all_skills]\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            self.precision = precision_score(true_labels, predicted_labels)\n",
    "            self.recall = recall_score(true_labels, predicted_labels)\n",
    "            self.f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "            # Save the results to a file\n",
    "            results = {\n",
    "                'top_skills': self.top_skills,\n",
    "                'precision': self.precision,\n",
    "                'recall': self.recall,\n",
    "                'f1': self.f1\n",
    "            }\n",
    "            with open(results_file, 'w') as file:\n",
    "                json.dump(results, file)\n",
    "\n",
    "    def create_skills_analysis_tab(self):\n",
    "        # Create a frame for the top 5 skills\n",
    "        top_skills_frame = ttk.Frame(self.tab)\n",
    "        top_skills_frame.pack(pady=10)\n",
    "\n",
    "        # Display the top 5 skills\n",
    "        top_skills_label = ttk.Label(top_skills_frame, text=\"Top 5 Skills in Demand:\", font=(\"Arial\", 16))\n",
    "        top_skills_label.pack()\n",
    "\n",
    "        for skill, count in self.top_skills:\n",
    "            skill_label = ttk.Label(top_skills_frame, text=f\"{skill}: {count}\", font=(\"Arial\", 12))\n",
    "            skill_label.pack()\n",
    "\n",
    "        # Create a frame for the bar graph\n",
    "        graph_frame = ttk.Frame(self.tab)\n",
    "        graph_frame.pack(pady=10)\n",
    "\n",
    "        # Create a bar graph of the top 5 skills and their frequencies\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        skills, counts = zip(*self.top_skills)\n",
    "        ax.bar(skills, counts)\n",
    "        ax.set_xlabel(\"Skills\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(\"Top 5 Skills in Demand\")\n",
    "\n",
    "        # Rotate x-axis labels if needed\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Adjust layout to prevent overlapping\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Create a canvas for the graph\n",
    "        canvas = FigureCanvasTkAgg(fig, master=graph_frame)\n",
    "        canvas.draw()\n",
    "\n",
    "        # Pack the canvas\n",
    "        canvas.get_tk_widget().pack()\n",
    "\n",
    "        # Create a frame for the advanced button and evaluation metrics\n",
    "        self.advanced_frame = ttk.Frame(self.tab)\n",
    "        self.advanced_frame.pack(pady=10)\n",
    "\n",
    "        # Create an advanced button\n",
    "        self.advanced_button = ttk.Button(self.advanced_frame, text=\"Advanced\", command=self.toggle_evaluation_metrics)\n",
    "        self.advanced_button.pack()\n",
    "\n",
    "        # Create a frame for the evaluation metrics\n",
    "        self.metrics_frame = ttk.Frame(self.advanced_frame)\n",
    "\n",
    "    def toggle_evaluation_metrics(self):\n",
    "        if self.metrics_visible:\n",
    "            self.metrics_frame.pack_forget()  # Hide the evaluation metrics\n",
    "            self.metrics_visible = False\n",
    "        else:\n",
    "            self.show_evaluation_metrics()  # Show the evaluation metrics\n",
    "            self.metrics_visible = True\n",
    "\n",
    "    def show_evaluation_metrics(self):\n",
    "        # Clear the existing evaluation metrics frame\n",
    "        for widget in self.metrics_frame.winfo_children():\n",
    "            widget.destroy()\n",
    "\n",
    "        # Create labels for the evaluation metrics\n",
    "        precision_label = ttk.Label(self.metrics_frame, text=f\"Precision: {self.precision:.4f}\")\n",
    "        recall_label = ttk.Label(self.metrics_frame, text=f\"Recall: {self.recall:.4f}\")\n",
    "        f1_label = ttk.Label(self.metrics_frame, text=f\"F1-Score: {self.f1:.4f}\")\n",
    "\n",
    "        # Pack the labels\n",
    "        precision_label.pack()\n",
    "        recall_label.pack()\n",
    "        f1_label.pack()\n",
    "\n",
    "        # Pack the evaluation metrics frame\n",
    "        self.metrics_frame.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8230617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketAnalysis:\n",
    "    def __init__(self, api_key):\n",
    "        self.fred = Fred(api_key=api_key)\n",
    "        self.start_date = '1990-01-01'\n",
    "        self.end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "        self.indicators = ['AMVPNO', 'CES3133600101', 'CUUR0000SETC', 'DMOTRC1Q027SBEA', 'RSMVPD']\n",
    "        self.data_df = self.fetch_data()\n",
    "        self.X, self.y = self.prepare_data()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.split_data()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.X_train_scaled = self.preprocess_data(self.X_train)\n",
    "        self.X_test_scaled = self.preprocess_data(self.X_test)\n",
    "        self.nn_model = self.build_nn_model()\n",
    "        self.rf_model = self.build_rf_model()\n",
    "        self.train_nn_model()\n",
    "        self.train_rf_model()\n",
    "        self.nn_report, self.rf_report, self.latest_category = self.evaluate_models()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        data_df = pd.DataFrame()\n",
    "        for indicator in self.indicators:\n",
    "            data = self.fred.get_series(indicator, observation_start=self.start_date, observation_end=self.end_date, frequency='q')\n",
    "            data_df[indicator] = data.ffill().bfill()\n",
    "        np.random.seed(42)\n",
    "        data_df['GrowthCategory'] = np.random.choice([0, 1, 2, 3], size=len(data_df))\n",
    "        data_df[self.indicators] = data_df[self.indicators].pct_change().fillna(0)\n",
    "        return data_df\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X = self.data_df[self.indicators].values\n",
    "        y = self.data_df['GrowthCategory'].values\n",
    "        return X, y\n",
    "\n",
    "    def split_data(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.25, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def preprocess_data(self, X):\n",
    "        X_scaled = self.scaler.fit_transform(self.imputer.fit_transform(X))\n",
    "        return X_scaled\n",
    "\n",
    "    def build_nn_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(128, input_dim=self.X_train_scaled.shape[1]),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_nn_model(self):\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train_scaled, self.y_train)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.nn_model.fit(X_train_resampled, to_categorical(y_train_resampled), epochs=200, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "    def build_rf_model(self):\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "        return grid_search\n",
    "\n",
    "    def train_rf_model(self):\n",
    "        self.rf_model.fit(self.X_train_scaled, self.y_train)\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        nn_predictions = np.argmax(self.nn_model.predict(self.X_test_scaled), axis=1)\n",
    "        nn_report = classification_report(self.y_test, nn_predictions)\n",
    "        best_rf = self.rf_model.best_estimator_\n",
    "        rf_predictions = best_rf.predict(self.X_test_scaled)\n",
    "        rf_report = classification_report(self.y_test, rf_predictions)\n",
    "        latest_data = self.data_df.iloc[-1]\n",
    "        latest_indicators = latest_data[self.indicators].values.reshape(1, -1)\n",
    "        latest_scaled_data = self.scaler.transform(self.imputer.transform(latest_indicators))\n",
    "        latest_category = np.argmax(self.nn_model.predict(latest_scaled_data), axis=1)[0]\n",
    "        return nn_report, rf_report, latest_category\n",
    "\n",
    "    def save_results(self, filename='classification_results.pkl'):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump((self.nn_report, self.rf_report, self.latest_category), f)\n",
    "\n",
    "    def load_results(self, filename='classification_results.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.nn_report, self.rf_report, self.latest_category = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92487a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobPostingsAnalysis:\n",
    "    def __init__(self, filepath='job_postings_by_sector_US.csv'):\n",
    "        self.filepath = filepath\n",
    "        self.job_postings = pd.read_csv(filepath, parse_dates=['date'])\n",
    "        self.unique_jobs = self.job_postings['display_name'].unique()\n",
    "        self.historical_data_sp500 = None\n",
    "        self.historical_data_btc = None\n",
    "        self.treasury_inflation_expectations = None\n",
    "        self.inflation_expectations_5_years = None\n",
    "        self.nominal_broad_dollar_index = None\n",
    "        self.job_postings_indeed = None\n",
    "        self.new_job_postings_indeed = None\n",
    "        self.treasury_spread = None\n",
    "        self.federal_funds_rate = None\n",
    "        self.treasury_yield_10y = None\n",
    "        self.equity_market_volatility = None\n",
    "        self.high_yield_index_yield = None\n",
    "        self.bank_prime_loan_rate = None\n",
    "\n",
    "\n",
    "    def load_additional_data(self, fred_api_key):\n",
    "        # Initialize the FRED API\n",
    "        fred = Fred(api_key=fred_api_key)\n",
    "        end_date = '2024-03-22'\n",
    "        end_date_yfinance = '2024-03-23'\n",
    "        \n",
    "        # Load Job Postings on Indeed in the United States (IHLIDXUS)\n",
    "        series_id = 'IHLIDXUS'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.job_postings_indeed = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load New Job Postings on Indeed in the United States (IHLIDXNEWUS)\n",
    "        series_id = 'IHLIDXNEWUS'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.new_job_postings_indeed = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity\n",
    "        series_id = 'T10Y2Y'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.treasury_spread = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load Federal Funds Effective Rate\n",
    "        series_id = 'DFF'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.federal_funds_rate = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load Market Yield on U.S. Treasury Securities at 10-Year Constant Maturity, Quoted on an Investment Basis\n",
    "        series_id = 'DGS10'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.treasury_yield_10y = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load Equity Market Volatility: Infectious Disease Tracker\n",
    "        series_id = 'INFECTDISEMVTRACKD'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.equity_market_volatility = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load ICE BofA CCC & Lower US High Yield Index Effective Yield\n",
    "        series_id = 'BAMLH0A3HYCEY'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.high_yield_index_yield = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load Bank Prime Loan Rate\n",
    "        series_id = 'RIFSPBLPND'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.bank_prime_loan_rate = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "\n",
    "        # Load and prepare the Treasury Inflation Expectations data\n",
    "        series_id = 'T10YIE'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.treasury_inflation_expectations = pd.DataFrame(data, columns=[series_id])\n",
    "        self.treasury_inflation_expectations['ds'] = self.treasury_inflation_expectations.index\n",
    "        self.treasury_inflation_expectations.rename(columns={series_id: 'treasury_inflation_expectations'}, inplace=True)\n",
    "\n",
    "        # Load and prepare the 5-Year Inflation Expectations data\n",
    "        series_id = 'T5YIFR'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.inflation_expectations_5_years = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load and prepare the Nominal Broad U.S. Dollar Index data\n",
    "        series_id = 'DTWEXBGS'\n",
    "        data = fred.get_series(series_id, '2020-02-01', end_date)\n",
    "        self.nominal_broad_dollar_index = pd.DataFrame(data, columns=[series_id])\n",
    "\n",
    "        # Load S&P 500 data\n",
    "        sp500 = yf.Ticker('^GSPC')\n",
    "        self.historical_data_sp500 = sp500.history(start='2020-02-01', end=end_date_yfinance)[['Close']]\n",
    "        self.historical_data_sp500.index = pd.to_datetime(self.historical_data_sp500.index.strftime('%Y-%m-%d'))\n",
    "\n",
    "        # Load Bitcoin data\n",
    "        btc_usd = yf.Ticker('BTC-USD')\n",
    "        self.historical_data_btc = btc_usd.history(start='2020-02-01', end=end_date_yfinance)[['Close']]\n",
    "        self.historical_data_btc.index = pd.to_datetime(self.historical_data_btc.index.strftime('%Y-%m-%d'))\n",
    "\n",
    "    def get_job_sector_data(self, sector):\n",
    "        # Filter job postings for the specified sector\n",
    "        data = self.job_postings[self.job_postings['display_name'] == sector][['date', 'indeed_job_postings_index']]\n",
    "        data.rename(columns={'date': 'ds', 'indeed_job_postings_index': 'y'}, inplace=True)\n",
    "        data['ds'] = pd.to_datetime(data['ds'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "        # Merge with S&P 500 data\n",
    "        data = data.merge(self.historical_data_sp500, how='left', left_on='ds', right_index=True)\n",
    "        data.rename(columns={'Close': 'sp500'}, inplace=True)\n",
    "\n",
    "        # Merge with Bitcoin data\n",
    "        data = data.merge(self.historical_data_btc, how='left', left_on='ds', right_index=True)\n",
    "        data.rename(columns={'Close': 'btc_usd'}, inplace=True)\n",
    "\n",
    "        # Merge with Treasury Inflation Expectations data\n",
    "        data = data.merge(self.treasury_inflation_expectations, how='left', on='ds')\n",
    "\n",
    "        # Merge with 5-Year Inflation Expectations data\n",
    "        data = data.merge(self.inflation_expectations_5_years, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with Nominal Broad U.S. Dollar Index data\n",
    "        data = data.merge(self.nominal_broad_dollar_index, how='left', left_on='ds', right_index=True)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        # Merge with Job Postings on Indeed in the United States (IHLIDXUS)\n",
    "        data = data.merge(self.job_postings_indeed, how='left', left_on='ds', right_index=True)\n",
    "        data.rename(columns={'Close': 'IHLIDXUS'}, inplace=True)\n",
    "        # Merge with New Job Postings on Indeed in the United States (IHLIDXNEWUS)\n",
    "        data = data.merge(self.new_job_postings_indeed, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity\n",
    "        data = data.merge(self.treasury_spread, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with Federal Funds Effective Rate\n",
    "        data = data.merge(self.federal_funds_rate, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with Market Yield on U.S. Treasury Securities at 10-Year Constant Maturity, Quoted on an Investment Basis\n",
    "        data = data.merge(self.treasury_yield_10y, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with Equity Market Volatility: Infectious Disease Tracker\n",
    "        data = data.merge(self.equity_market_volatility, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with ICE BofA CCC & Lower US High Yield Index Effective Yield\n",
    "        data = data.merge(self.high_yield_index_yield, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        # Merge with Bank Prime Loan Rate\n",
    "        data = data.merge(self.bank_prime_loan_rate, how='left', left_on='ds', right_index=True)\n",
    "\n",
    "        \n",
    "        ###\n",
    "\n",
    "        # Split data before filling missing values\n",
    "        split_percentage = 0.8\n",
    "        split_point = int(len(data) * split_percentage)\n",
    "        training_data = data[:split_point]\n",
    "        testing_data = data[split_point:]\n",
    "\n",
    "        # Fill missing values in training data\n",
    "        training_data.fillna(method='ffill', inplace=True)\n",
    "        training_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "        # Use the last value of the training data to fill initial NaNs in testing data\n",
    "        for column in testing_data.columns:\n",
    "            if pd.isnull(testing_data[column].iloc[0]):\n",
    "                testing_data[column].iloc[0] = training_data[column].iloc[-1]\n",
    "\n",
    "        # Then, forward fill the remaining NaNs in testing data\n",
    "        testing_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Check for any NaN values in the training and testing data\n",
    "        if training_data.isnull().any().any():\n",
    "            raise ValueError(\"NaN values found in training data after processing.\")\n",
    "        if testing_data.isnull().any().any():\n",
    "            raise ValueError(\"NaN values found in testing data after processing.\")\n",
    "\n",
    "        # Combine the data back\n",
    "        data = pd.concat([training_data, testing_data])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def perform_cross_validation(self, model, data):\n",
    "        # Parameters for cross-validation\n",
    "        cv_horizon = '90 days'  # Forecast horizon\n",
    "        cv_initial = '180 days'  # Initial training period should capture at least one full seasonal cycle\n",
    "        cv_period = '60 days'  # Spacing between cutoff dates for a good balance between granularity and computational efficiency\n",
    "\n",
    "        # Perform cross-validation\n",
    "        df_cv = cross_validation(model, initial=cv_initial, period=cv_period, horizon=cv_horizon)\n",
    "\n",
    "        # Compute performance metrics\n",
    "        df_p = performance_metrics(df_cv)\n",
    "\n",
    "        # Return the aggregated RMSE (or another metric of your choice)\n",
    "        return df_p['rmse'].mean()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize_hyperparameters(self, data):  \n",
    "    \n",
    "        def drop_highly_correlated(df, threshold):\n",
    "            # Create correlation matrix\n",
    "            corr_matrix = df.corr().abs()\n",
    "            # Select upper triangle of correlation matrix\n",
    "            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "            # Find index of feature columns with correlation greater than the threshold\n",
    "            to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "            # Drop features\n",
    "            df = df.drop(df[to_drop], axis=1)\n",
    "            return df\n",
    "    \n",
    "        best_mae_train = float('inf')\n",
    "        best_mae_test = float('inf')\n",
    "        best_scale = 0.05\n",
    "        best_regressors = None\n",
    "        best_model = None\n",
    "        best_forecast = None\n",
    "        split_percentage = 0.8\n",
    "        best_rmse = float('inf')\n",
    "        best_test_mae = 0 # THIS IS A TEST\n",
    "        best_train_mae = 9999 #THIS IS A TEST\n",
    "        best_params = {}\n",
    "\n",
    "\n",
    "        regressors = ['sp500', 'btc_usd', 'treasury_inflation_expectations', 'T5YIFR', 'DTWEXBGS', 'IHLIDXUS', 'IHLIDXNEWUS', 'T10Y2Y', 'DFF', 'DGS10', 'INFECTDISEMVTRACKD', 'BAMLH0A3HYCEY', 'RIFSPBLPND']\n",
    "        \n",
    "        data_with_regressors = data[['ds'] + regressors]\n",
    "        #print(data_with_regressors)\n",
    "        data_with_regressors = drop_highly_correlated(data_with_regressors, threshold=0.6)\n",
    "        filtered_regressors = list(data_with_regressors.columns)\n",
    "        filtered_regressors.remove('ds')\n",
    "\n",
    "        regressor_combinations = sum([list(itertools.combinations(filtered_regressors, i)) for i in range(1, len(filtered_regressors) + 1)], [])\n",
    "        \n",
    "        for combination in regressor_combinations:\n",
    "            changepoint_prior_scale_value = 10\n",
    "            previous_mae = float('inf')\n",
    "            iterations = 0\n",
    "\n",
    "            while changepoint_prior_scale_value >= 0.001 and iterations < 5:  # Increased the minimum value and reduced iterations\n",
    "                # Initialize the model\n",
    "                model = Prophet(changepoint_prior_scale=changepoint_prior_scale_value, n_changepoints=5)\n",
    "\n",
    "                # Remove other seasonality components\n",
    "                model.add_seasonality(name='weekly', period=7, fourier_order=3)\n",
    "                model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "\n",
    "                #manually add customized seasonalities with limited Fourier terms\n",
    "                model.add_seasonality(name='yearly', period=365.25, fourier_order=3)  # Fewer Fourier terms for yearly seasonality\n",
    "\n",
    "\n",
    "                # Add regressors\n",
    "                for regressor in combination:\n",
    "                    model.add_regressor(regressor)\n",
    "\n",
    "                # Split data\n",
    "                split_point = int(len(data) * split_percentage)\n",
    "                training_data = data[:split_point]\n",
    "                testing_data = data[split_point:]\n",
    "\n",
    "                # Fit the model\n",
    "                model.fit(training_data)\n",
    "\n",
    "                # Perform cross-validation and get the average RMSE\n",
    "                avg_rmse = self.perform_cross_validation(model, data)\n",
    "\n",
    "                # Create future dataframe\n",
    "                future = model.make_future_dataframe(periods=len(testing_data))\n",
    "                for regressor in combination:\n",
    "                    future = future.merge(data[['ds', regressor]], on='ds', how='left')\n",
    "\n",
    "                # Predict the future\n",
    "                forecast = model.predict(future)\n",
    "\n",
    "                # Evaluate the model\n",
    "                y_true_train = training_data['y'].values\n",
    "                y_pred_train = forecast.iloc[:split_point]['yhat'].values\n",
    "                mae_train = mean_absolute_error(y_true_train, y_pred_train)\n",
    "\n",
    "                y_true_test = testing_data['y'].values\n",
    "                y_pred_test = forecast.iloc[split_point:]['yhat'].values\n",
    "                mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "\n",
    "                # Calculate MAPE for training and testing\n",
    "                mape_train = np.mean(np.abs((y_true_train - y_pred_train) / y_true_train)) * 100\n",
    "                mape_test = np.mean(np.abs((y_true_test - y_pred_test) / y_true_test)) * 100\n",
    "\n",
    "                # Calculate RMSE for training and testing\n",
    "                rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "                rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "\n",
    "\n",
    "                # Compare the average RMSE to find the best model\n",
    "                if avg_rmse < best_rmse: \n",
    "                    best_test_mae = mae_test\n",
    "                    best_train_mae = mae_train\n",
    "                    best_rmse = avg_rmse\n",
    "                    best_mae_train = mae_train\n",
    "                    best_mae_test = mae_test\n",
    "                    best_scale = changepoint_prior_scale_value\n",
    "                    best_regressors = combination\n",
    "                    best_model = model\n",
    "                    best_forecast = forecast\n",
    "                    best_mape_train = mape_train\n",
    "                    best_mape_test = mape_test\n",
    "                    best_rmse_train = rmse_train\n",
    "                    best_rmse_test = rmse_test\n",
    "\n",
    "                # Adaptive step size based on improvement\n",
    "                if (previous_mae - mae_test) / previous_mae < 0.01:\n",
    "                    changepoint_prior_scale_value *= 0.5\n",
    "                    iterations += 1\n",
    "                else:\n",
    "                    previous_mae = mae_test\n",
    "\n",
    "                changepoint_prior_scale_value -= changepoint_prior_scale_value * 0.5\n",
    "\n",
    "        ###\n",
    "\n",
    "        data_dict = {\n",
    "            'sp500': self.historical_data_sp500,\n",
    "            'btc_usd': self.historical_data_btc,\n",
    "            'treasury_inflation_expectations': self.treasury_inflation_expectations,\n",
    "            'T5YIFR': self.inflation_expectations_5_years,\n",
    "            'DTWEXBGS': self.nominal_broad_dollar_index,\n",
    "            'IHLIDXUS': self.job_postings_indeed,\n",
    "            'IHLIDXNEWUS': self.new_job_postings_indeed,\n",
    "            'T10Y2Y': self.treasury_spread,\n",
    "            'DFF': self.federal_funds_rate,\n",
    "            'DGS10': self.treasury_yield_10y,\n",
    "            'INFECTDISEMVTRACKD': self.equity_market_volatility,\n",
    "            'BAMLH0A3HYCEY': self.high_yield_index_yield,\n",
    "            'RIFSPBLPND': self.bank_prime_loan_rate\n",
    "        }\n",
    "        \n",
    "        # Initialize an empty dictionary to hold the forecasts\n",
    "        forecasts_dict = {}\n",
    "        merged_df = pd.DataFrame()\n",
    "\n",
    "        for regressor, data in data_dict.items():\n",
    "            # Ensure the data starts from 2020-02-01\n",
    "            start_date = pd.Timestamp('2020-02-01')\n",
    "            end_date = data.index.max()\n",
    "            \n",
    "            # Create a date range from 2020-02-01 to the end of the dataset\n",
    "            full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "            data = data.reindex(full_date_range).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "            print(data.head())\n",
    "            # Split the data into training and test sets\n",
    "            split_percentage = 0.8\n",
    "            split_point = int(len(data) * split_percentage)\n",
    "\n",
    "            \n",
    "            train = data[:split_point]\n",
    "            test = data[split_point:]\n",
    "\n",
    "            # Reset the index to move 'Date' from the index to a column\n",
    "            train = train.reset_index()\n",
    "            test = test.reset_index()\n",
    "\n",
    "            # Get the last column name (the one to be predicted)\n",
    "            # Assuming df is your DataFrame\n",
    "            if 'ds' in train.columns:\n",
    "                train.reset_index(drop=True)\n",
    "                train = train.set_index('ds')\n",
    "                test.reset_index(drop=True)\n",
    "                test = test.set_index('ds')\n",
    "\n",
    "            train.columns = ['ds', 'y']\n",
    "            test.columns = ['ds', 'y']\n",
    "            # Rename the 'Date' column to 'ds' and the target column to 'y'\n",
    "            #train.rename(columns={'Date': 'ds', target_column: 'y'}, inplace=True)\n",
    "            #test.rename(columns={'Date': 'ds', target_column: 'y'}, inplace=True)\n",
    "            \n",
    "            # Ensure that 'ds' is of datetime type\n",
    "            train['ds'] = pd.to_datetime(train['ds'])\n",
    "            test['ds'] = pd.to_datetime(test['ds'])\n",
    "\n",
    "            # Initialize and fit the Prophet model\n",
    "            model = Prophet(changepoint_prior_scale=0.5)\n",
    "            model.fit(pd.concat([train, test]))\n",
    "\n",
    "            # Create a dataframe for future predictions\n",
    "            future = model.make_future_dataframe(periods=90)\n",
    "\n",
    "            # Make the predictions\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            # Store the forecast in the dictionary\n",
    "            forecasts_dict[regressor] = forecast['yhat']\n",
    "            # Add the regressor name as a new column in the forecast DataFrame\n",
    "            forecast = forecast[['ds', 'yhat']]\n",
    "            forecast.rename(columns={'yhat':regressor}, inplace=True)\n",
    "\n",
    "            # If merged_df is empty, assign the forecast DataFrame to it\n",
    "            #print(regressor)\n",
    "            #print(\"---------------------FORECAST 2----------------------\")\n",
    "            #print(forecast)\n",
    "            #print(\"---------------------FORECAST 2----------------------\")\n",
    "            if merged_df.empty:\n",
    "                merged_df = forecast\n",
    "            else:\n",
    "                # Merge the current forecast DataFrame into the merged_df DataFrame on 'ds'\n",
    "                merged_df = pd.merge(merged_df, forecast, on='ds', how='outer')\n",
    "\n",
    "        merged_df['ds'] = pd.to_datetime(merged_df['ds'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        \n",
    "        # Prepare the future dataframe for prediction\n",
    "        extended_future = best_model.make_future_dataframe(periods=len(testing_data) + 90)\n",
    "        # Select the columns to merge based on the regressors list\n",
    "        columns_to_merge = ['ds'] + regressors\n",
    "\n",
    "        # Merge the selected columns from merged_df to extended_future\n",
    "        extended_future = extended_future.merge(merged_df[columns_to_merge], on='ds', how='left')\n",
    "        \n",
    "        extended_future.rename(columns={'T10YIE': 'treasury_inflation_expectations'}, inplace=True)\n",
    "\n",
    "        # Predict the future\n",
    "        extended_forecast = best_model.predict(extended_future)\n",
    "        #print(\"EXTENDED FORECAST\")\n",
    "        #print(extended_forecast)\n",
    "\n",
    "        return {\n",
    "                'avg_rmse': best_rmse,  # Include the best average RMSE here\n",
    "                'scale': best_scale,\n",
    "                'mae_train': best_mae_train,\n",
    "                'mae_test': best_mae_test,\n",
    "                'mape_train': best_mape_train,\n",
    "                'mape_test': best_mape_test,\n",
    "                'rmse_train': best_rmse_train,\n",
    "                'rmse_test': best_rmse_test,\n",
    "                'regressors': best_regressors,\n",
    "                'model': best_model,\n",
    "                'forecast': best_forecast,\n",
    "                'extended_forecast': extended_forecast,\n",
    "                'params': best_params,\n",
    "            }\n",
    "\n",
    "    def plot_job_postings_forecast(self, ax, sector):\n",
    "        # Load additional data if not already loaded\n",
    "        if self.historical_data_sp500 is None:\n",
    "            self.load_additional_data(fred_api_key='3fda1d45198afb430e77220ef20d9de0')\n",
    "\n",
    "        # Now you can safely use get_job_sector_data\n",
    "        data = self.get_job_sector_data(sector)\n",
    "\n",
    "        # Then, optimize the hyperparameters for the Prophet model based on the sector data\n",
    "        best_scale, best_mae_train, best_mae_test, best_regressors, best_model, best_forecast = self.optimize_hyperparameters(data)\n",
    "\n",
    "        # Plotting the observed values\n",
    "        #ax.plot(data['ds'], data['y'], label='Observed', color='blue')\n",
    "\n",
    "        # Plotting the forecasted values\n",
    "        ax.plot(best_forecast['ds'], best_forecast['yhat'], label='Forecasted', color='orange')\n",
    "\n",
    "        # Filling the area between the upper and lower confidence intervals\n",
    "        ax.fill_between(best_forecast['ds'], best_forecast['yhat_lower'], best_forecast['yhat_upper'], color='gray', alpha=0.2)\n",
    "\n",
    "        # Setting labels and title\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Job Postings Index')\n",
    "        ax.set_title(f'Forecasted Job Postings Index for {sector}')\n",
    "        ax.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def run_analysis_for_all_sectors(self, valid_sectors):\n",
    "        self.best_parameters = {}\n",
    "        self.load_additional_data(fred_api_key='3fda1d45198afb430e77220ef20d9de0')  # Ensure data is loaded\n",
    "        for sector in valid_sectors:\n",
    "            # Ensure data for each sector is processed\n",
    "            if not os.path.exists(f'best_parameters_{sector}.joblib'):\n",
    "                data = self.get_job_sector_data(sector)\n",
    "                results = self.optimize_hyperparameters(data)\n",
    "                self.best_parameters[sector] = results\n",
    "                joblib.dump(results, f'best_parameters_{sector}.joblib')\n",
    "            else:\n",
    "                results = joblib.load(f'best_parameters_{sector}.joblib')\n",
    "\n",
    "            data = self.get_job_sector_data(sector)\n",
    "            self.best_parameters[sector] = {\n",
    "                'avg_rmse': results['avg_rmse'],  # Include the best average RMSE here\n",
    "                'scale': results['scale'],\n",
    "                'mae_train': results['mae_train'],\n",
    "                'mae_test': results['mae_test'],\n",
    "                'mape_train': results['mape_train'],  # Ensure this key is correctly named\n",
    "                'mape_test': results['mape_test'],\n",
    "                'rmse_train': results['rmse_train'],\n",
    "                'rmse_test': results['rmse_test'],\n",
    "                'regressors': results['regressors'],\n",
    "                'model': results['model'],\n",
    "                'forecast': results['forecast'],\n",
    "                'extended_forecast': results['extended_forecast'],\n",
    "            }\n",
    "\n",
    "            # Save the model and parameters to a file\n",
    "            joblib.dump(self.best_parameters[sector], f'best_parameters_{sector}.joblib')\n",
    "            print(f\"The best average RMSE for {sector} is: {results['avg_rmse']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_best_parameters_for_sector(self, sector):\n",
    "        return self.best_parameters.get(sector, None)\n",
    "    \n",
    "    \n",
    "    def compare_last_values(self, sector):\n",
    "        best_parameters = self.get_best_parameters_for_sector(sector)\n",
    "        if best_parameters is not None:\n",
    "            # Get the last value in the test data\n",
    "            last_test_value = best_parameters['forecast']['yhat'].iloc[-1]\n",
    "\n",
    "            # Get the last value in the extended forecast\n",
    "            last_forecast_value = best_parameters['extended_forecast']['yhat'].iloc[-1]\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            percentage_difference = ((last_forecast_value - last_test_value) / last_test_value) * 100\n",
    "\n",
    "            # Compare the percentage difference with 2%\n",
    "            if percentage_difference > 2:\n",
    "                return \"There will be a positive impact on job posting index\"\n",
    "            elif percentage_difference < -2:\n",
    "                return \"There will be a negative impact on job posting index\"\n",
    "            else:\n",
    "                return \"Job posting index will be stable\"\n",
    "        else:\n",
    "            return \"No data available for this sector\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468218cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_data.fillna(method='ffill', inplace=True)\n",
      "11:58:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "Seasonality has period of 365.25 days which is larger than initial window. Consider increasing initial.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aa9fb9b7a74298a9ec23b43f77feed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:58:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:306: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if (previous_mae - mae_test) / previous_mae < 0.01:\n",
      "11:58:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "Seasonality has period of 365.25 days which is larger than initial window. Consider increasing initial.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f819349ece4eb9bb123c4257a80f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:58:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:306: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if (previous_mae - mae_test) / previous_mae < 0.01:\n",
      "11:59:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "Seasonality has period of 365.25 days which is larger than initial window. Consider increasing initial.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afba62ed684e48aeb47c3499bc6e6715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:306: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  if (previous_mae - mae_test) / previous_mae < 0.01:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Close\n",
      "2020-02-01  3248.919922\n",
      "2020-02-02  3248.919922\n",
      "2020-02-03  3248.919922\n",
      "2020-02-04  3297.590088\n",
      "2020-02-05  3334.689941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Close\n",
      "2020-02-01  9392.875000\n",
      "2020-02-02  9344.365234\n",
      "2020-02-03  9293.521484\n",
      "2020-02-04  9180.962891\n",
      "2020-02-05  9613.423828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            treasury_inflation_expectations         ds\n",
      "2020-02-01                             1.63 2020-02-03\n",
      "2020-02-02                             1.63 2020-02-03\n",
      "2020-02-03                             1.63 2020-02-03\n",
      "2020-02-04                             1.64 2020-02-04\n",
      "2020-02-05                             1.66 2020-02-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            T5YIFR\n",
      "2020-02-01    1.70\n",
      "2020-02-02    1.70\n",
      "2020-02-03    1.70\n",
      "2020-02-04    1.70\n",
      "2020-02-05    1.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:19 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DTWEXBGS\n",
      "2020-02-01  116.1176\n",
      "2020-02-02  116.1176\n",
      "2020-02-03  116.1176\n",
      "2020-02-04  115.9290\n",
      "2020-02-05  116.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:21 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            IHLIDXUS\n",
      "2020-02-01    100.00\n",
      "2020-02-02     99.98\n",
      "2020-02-03     99.97\n",
      "2020-02-04    100.03\n",
      "2020-02-05    100.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:23 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            IHLIDXNEWUS\n",
      "2020-02-01       100.00\n",
      "2020-02-02       100.86\n",
      "2020-02-03       101.71\n",
      "2020-02-04       101.99\n",
      "2020-02-05       102.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:25 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            T10Y2Y\n",
      "2020-02-01    0.18\n",
      "2020-02-02    0.18\n",
      "2020-02-03    0.18\n",
      "2020-02-04    0.20\n",
      "2020-02-05    0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DFF\n",
      "2020-02-01  1.59\n",
      "2020-02-02  1.59\n",
      "2020-02-03  1.59\n",
      "2020-02-04  1.59\n",
      "2020-02-05  1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:29 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DGS10\n",
      "2020-02-01   1.54\n",
      "2020-02-02   1.54\n",
      "2020-02-03   1.54\n",
      "2020-02-04   1.61\n",
      "2020-02-05   1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:31 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            INFECTDISEMVTRACKD\n",
      "2020-02-01                8.36\n",
      "2020-02-02                3.20\n",
      "2020-02-03                8.18\n",
      "2020-02-04                5.50\n",
      "2020-02-05                2.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BAMLH0A3HYCEY\n",
      "2020-02-01          11.84\n",
      "2020-02-02          11.84\n",
      "2020-02-03          11.84\n",
      "2020-02-04          11.66\n",
      "2020-02-05          11.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:34 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            RIFSPBLPND\n",
      "2020-02-01        4.75\n",
      "2020-02-02        4.75\n",
      "2020-02-03        4.75\n",
      "2020-02-04        4.75\n",
      "2020-02-05        4.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:59:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best average RMSE for Administrative Assistance is: 62.385545396940024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\mosae\\AppData\\Local\\Temp\\ipykernel_22456\\4020076299.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sector: 'Administrative Assistance'\n",
      "Available keys: ['Administrative Assistance']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import re\n",
    "from prophet import Prophet\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from fredapi import Fred\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import tkinter as tk\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import random\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import spacy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import itertools\n",
    "import yfinance as yf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import nltk\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_skills_analysis_tab(parent):\n",
    "    skill_analysis_tab = SkillAnalysisTab(parent)\n",
    "\n",
    "# Function to create the Industry Analysis tab\n",
    "def create_industry_analysis_tab(parent):\n",
    "    # Create an instance of the MarketAnalysis class\n",
    "    market_analysis = MarketAnalysis(api_key='3fda1d45198afb430e77220ef20d9de0')\n",
    "\n",
    "    # Create a label for the latest category\n",
    "    latest_category_label = ttk.Label(parent, text=\"\")\n",
    "    latest_category_label.pack(side=tk.TOP)\n",
    "\n",
    "    # Create a frame for the content\n",
    "    content_frame = ttk.Frame(parent)\n",
    "    content_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    canvas = tk.Canvas(content_frame)\n",
    "    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    scrollbar = ttk.Scrollbar(content_frame, command=canvas.yview)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill='y')\n",
    "\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "    canvas.bind('<Configure>', lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\")))\n",
    "\n",
    "    frame = ttk.Frame(canvas)\n",
    "    canvas.create_window((0, 0), window=frame, anchor='nw')\n",
    "\n",
    "    # Neural Network Classification Report\n",
    "    nn_label = tk.Label(frame, text=\"Neural Network Classification Report:\")\n",
    "    nn_label.pack()\n",
    "    nn_text = tk.Text(frame, height=10, width=80)\n",
    "    nn_text.pack()\n",
    "\n",
    "    # Random Forest Classification Report\n",
    "    rf_label = tk.Label(frame, text=\"\\nRandom Forest Classification Report with Hyperparameter Tuning:\")\n",
    "    rf_label.pack()\n",
    "    rf_text = tk.Text(frame, height=10, width=80)\n",
    "    rf_text.pack()\n",
    "\n",
    "    # Update the GUI with the results\n",
    "    category_map = {0: \"Declining\", 1: \"Slow Growth\", 2: \"Moderate Growth\", 3: \"Fast Growth\"}\n",
    "    latest_category_text = category_map[market_analysis.latest_category]\n",
    "    latest_category_label.config(text=f\"Latest Category: {latest_category_text}\")\n",
    "    nn_text.delete('1.0', tk.END)\n",
    "    nn_text.insert(tk.END, market_analysis.nn_report)\n",
    "    rf_text.delete('1.0', tk.END)\n",
    "    rf_text.insert(tk.END, market_analysis.rf_report)\n",
    "\n",
    "\n",
    "\n",
    "def create_job_posting_index_tab(parent):\n",
    "    job_index_result_text = tk.StringVar()\n",
    "    job_index_result_label = tk.Label(parent, textvariable=job_index_result_text, font=('Helvetica', 10))\n",
    "    job_index_result_label.pack(pady=(5, 10))\n",
    "    #valid_sectors = [\"Administrative Assistance\", \"Accounting\", \"Beauty & Wellness\", \"Physicians & Surgeons\", \"Media & Communications\"]\n",
    "    valid_sectors = [\"Administrative Assistance\"]\n",
    "    #valid_sectors = [\"Administrative Assistance\"]\n",
    "    \n",
    "    analysis = JobPostingsAnalysis()\n",
    "    analysis.run_analysis_for_all_sectors(valid_sectors)\n",
    "\n",
    "    def update_job_postings(sector):\n",
    "        # Compare the last values and update the result text\n",
    "        result_text = analysis.compare_last_values(sector)\n",
    "        job_index_result_text.set(result_text)\n",
    "\n",
    "        # Update the top graph based on the selected sector\n",
    "        fig.clear()\n",
    "        best_parameters = analysis.get_best_parameters_for_sector(sector)\n",
    "        if best_parameters is not None:\n",
    "            data = analysis.get_job_sector_data(sector)  # Get the data for the selected sector\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.plot(data['ds'], data['y'], label='Actual', color='blue')\n",
    "            ax.plot(best_parameters['extended_forecast']['ds'], best_parameters['extended_forecast']['yhat'], label='Final Model', color='green')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Job Postings Index')\n",
    "            ax.set_title(f'Job Postings Index for {sector}')\n",
    "            ax.legend()\n",
    "            fig_canvas.draw()\n",
    "\n",
    "        # Update the bottom graph based on the selected sector\n",
    "        fig2.clear()\n",
    "        if best_parameters is not None:\n",
    "            split_percentage = 0.7\n",
    "            train_data = data[:int(len(data) * split_percentage)]\n",
    "            test_data = data[int(len(data) * split_percentage):]\n",
    "            ax2 = fig2.add_subplot(111)\n",
    "            ax2.plot(train_data['ds'], train_data['y'], label='Training Data', color='blue')\n",
    "            ax2.plot(best_parameters['forecast']['ds'][:len(train_data)], best_parameters['forecast']['yhat'][:len(train_data)], label='Best Model (Training)', color='orange')\n",
    "            ax2.plot(test_data['ds'], test_data['y'], label='Testing Data', color='green')\n",
    "            ax2.plot(best_parameters['forecast']['ds'][len(train_data):], best_parameters['forecast']['yhat'][len(train_data):], label='Best Model (Testing)', color='red')\n",
    "            ax2.set_xlabel('Date')\n",
    "            ax2.set_ylabel('Job Postings Index')\n",
    "            ax2.set_title(f'Best Model Performance for {sector}')\n",
    "            ax2.legend()\n",
    "            canvas2.draw()            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    job_sector_var = tk.StringVar()\n",
    "    job_sector_var.set('Select Sector')\n",
    "    tk.Label(parent, text=\"Choose a job sector:\").pack()\n",
    "    popupMenu = tk.OptionMenu(parent, job_sector_var, *valid_sectors, command=update_job_postings)\n",
    "    popupMenu.pack()\n",
    "\n",
    "    fig = plt.Figure(figsize=(8, 6), dpi=100)\n",
    "    fig_canvas = FigureCanvasTkAgg(fig, master=parent)  \n",
    "    fig_canvas.draw()\n",
    "    fig_canvas.get_tk_widget().pack()\n",
    "\n",
    "    # Advanced section\n",
    "    advanced_frame = tk.Frame(parent)  # Create a new frame for the advanced section\n",
    "\n",
    "    def show_advanced_info():\n",
    "        # Hide or show advanced information based on current state\n",
    "        if advanced_frame.winfo_ismapped():\n",
    "            advanced_frame.pack_forget()\n",
    "            advanced_button.configure(text=\"Advanced \")\n",
    "        else:\n",
    "            sector = job_sector_var.get()\n",
    "            best_parameters = analysis.get_best_parameters_for_sector(sector)\n",
    "            if best_parameters is not None:\n",
    "                try:\n",
    "                    training_mape_label.config(text=f\"Training MAPE: {best_parameters['mape_train']}%\")  # Use 'mape_train' here\n",
    "                    testing_mape_label.config(text=f\"Testing MAPE: {best_parameters['mape_test']}%\")\n",
    "                    training_rmse_label.config(text=f\"Training RMSE: {best_parameters['rmse_train']}\")\n",
    "                    testing_rmse_label.config(text=f\"Testing RMSE: {best_parameters['rmse_test']}\")\n",
    "                    training_mae_label.config(text=\"Training MAE: \" + str(best_parameters['mae_train']))\n",
    "                    testing_mae_label.config(text=\"Testing MAE: \" + str(best_parameters['mae_test']))\n",
    "                    regressors_list.delete(0, tk.END)  # Clear the listbox\n",
    "                    for regressor in best_parameters['regressors']:\n",
    "                        regressors_list.insert(tk.END, regressor)  # Insert each regressor into the listbox\n",
    "                except KeyError:\n",
    "                    print(f\"Key not found for sector: '{sector}'\")\n",
    "                    training_mape_label.config(text=\"\")\n",
    "                    testing_mape_label.config(text=\"\")\n",
    "                    training_rmse_label.config(text=\"\")\n",
    "                    testing_rmse_label.config(text=\"\")\n",
    "                    training_mae_label.config(text=\"\")\n",
    "                    testing_mae_label.config(text=\"\")\n",
    "                    regressors_list.delete(0, tk.END)  # Clear the listbox\n",
    "            advanced_frame.pack()\n",
    "            advanced_button.configure(text=\"Advanced \")\n",
    "\n",
    "            print(f\"Selected sector: '{sector}'\")  # Debug print\n",
    "            print(f\"Available keys: {list(analysis.best_parameters.keys())}\")  # Debug print\n",
    "\n",
    "        # Update the scroll region after a small delay\n",
    "        root.after(100, lambda: canvas.configure(scrollregion=canvas.bbox(\"all\")))\n",
    "\n",
    "\n",
    "\n",
    "    # Button to toggle advanced information\n",
    "    advanced_button = tk.Button(parent, text=\"Advanced \", command=show_advanced_info)\n",
    "    advanced_button.pack(pady=(10, 5))\n",
    "\n",
    "    # Frame for advanced information\n",
    "    advanced_frame = tk.Frame(parent)\n",
    "\n",
    "    # Initially hide advanced frame\n",
    "    advanced_frame.pack_forget()\n",
    "    \n",
    "    training_mape_label = tk.Label(parent, text=\"\")\n",
    "    training_mape_label.pack()\n",
    "\n",
    "    testing_mape_label = tk.Label(parent, text=\"\")\n",
    "    testing_mape_label.pack()\n",
    "\n",
    "    training_rmse_label = tk.Label(parent, text=\"\")\n",
    "    training_rmse_label.pack()\n",
    "\n",
    "    testing_rmse_label = tk.Label(parent, text=\"\")\n",
    "    testing_rmse_label.pack()\n",
    "\n",
    "    # Text labels for training and testing MAE\n",
    "    training_mae_label = tk.Label(advanced_frame, text=\"Training MAE: 0.25\")\n",
    "    training_mae_label.pack()\n",
    "    testing_mae_label = tk.Label(advanced_frame, text=\"Testing MAE: 0.30\")\n",
    "    testing_mae_label.pack()\n",
    "\n",
    "    # List for the best regressors\n",
    "    best_regressors_label = tk.Label(advanced_frame, text=\"Best Regressors:\")\n",
    "    best_regressors_label.pack()\n",
    "    regressors_list = tk.Listbox(advanced_frame)\n",
    "    regressors_list.insert(1, \"Regressor 1\")\n",
    "    regressors_list.insert(2, \"Regressor 2\")\n",
    "    regressors_list.insert(3, \"Regressor 3\")\n",
    "    regressors_list.pack()\n",
    "\n",
    "    # Another line graph\n",
    "    fig2 = plt.Figure(figsize=(8, 6), dpi=100)\n",
    "    plot2 = fig2.add_subplot(111)\n",
    "    plot2.plot([1, 2, 3, 4], [50, 60, 70, 80])\n",
    "    canvas2 = FigureCanvasTkAgg(fig2, master=advanced_frame)\n",
    "    canvas2.draw()\n",
    "    canvas2.get_tk_widget().pack()\n",
    "\n",
    "\n",
    "    \n",
    "def program_not_done(tab):\n",
    "    label = ttk.Label(tab, text=\"Due to time constraints this program wasn't finished\", font=(\"Arial\", 20))\n",
    "    label.pack(expand=True, padx=20, pady=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main Tkinter window setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Market Analysis Dashboard\")\n",
    "root.geometry(\"1000x800\")\n",
    "root.resizable(width=0, height=0)\n",
    "\n",
    "canvas = tk.Canvas(root)\n",
    "canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "scrollbar = ttk.Scrollbar(root, command=canvas.yview)\n",
    "scrollbar.pack(side=tk.RIGHT, fill='y')\n",
    "\n",
    "canvas.configure(yscrollcommand=scrollbar.set)\n",
    "canvas.bind('<Configure>', lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\")))\n",
    "\n",
    "frame = ttk.Frame(canvas)\n",
    "canvas.create_window((90, 0), window=frame, anchor='nw')\n",
    "\n",
    "tab_control = ttk.Notebook(frame)\n",
    "tab1 = ttk.Frame(tab_control)\n",
    "tab2 = ttk.Frame(tab_control)\n",
    "tab3 = ttk.Frame(tab_control)\n",
    "\n",
    "# Assuming create_skills_analysis_tab and create_industry_analysis_tab are defined\n",
    "create_skills_analysis_tab(tab1)\n",
    "program_not_done(tab2)\n",
    "create_job_posting_index_tab(tab3)\n",
    "\n",
    "tab_control.add(tab1, text='Skills Analysis')\n",
    "tab_control.add(tab2, text='Industry Analysis')\n",
    "tab_control.add(tab3, text='Job Posting Index')\n",
    "\n",
    "tab_control.pack(expand=1, fill='both')\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b620d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ae0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
